{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "# Intuition\n",
    "\n",
    "Converting words into vectors that contain semantic meaning and relationships\n",
    "\n",
    "> We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity\n",
    "\n",
    "semantic embeddings have close vector distance to similar words. Word2Vec has problems, since a word can have different meanings. if we map from one word to a single embedding, there may be another word that is the exact same, but not simliar meaning\n",
    "\n",
    "I want to play at theatre\n",
    "\n",
    "John wants to play with his friends\n",
    "\n",
    "This is a limitation on search and comparison, because the same words may be close in distance but have very different meanings. Solution is to design a language model around the corpus or available text it is trained on, so it knows how words an be mapped together.\n",
    "\n",
    "Embedding models convert words to vectors.\n",
    "\n",
    "- First, embedding models project words into D-dimensional space in a hidden layer. An embedding is created which is a D-dimensional vector output from the hidden layer\n",
    "\n",
    "# Model Architectures\n",
    "\n",
    "Distributed representation of words are important\n",
    "\n",
    "## FeedForward Neural Net Language Model\n",
    "\n",
    "1. Input layer of dimension N, where we pass in a one hot encoded vector of dimension V into projection layer N x D. So 1xV => 1xD\n",
    "2. Then pass into hidden layer D x V. So each embedding will pass into the layer and return a 1xH vector\n",
    "3. Pass into output layer H x V. This will return a 1xV vector\n",
    "\n",
    "## Recurrent Neural Net Language Model\n",
    "\n",
    "- he need to specify the context length\n",
    "  RNNs are better at word modeling, connects hidden layers to itself,\n",
    "\n",
    "# New Log Linear Models\n",
    "\n",
    "Find models that might not be as good, but can be trained more efficiently\n",
    "\n",
    "Bag Of Words\n",
    "\n",
    "- remove the hidden layer, and pass the input layer directly to the output layer\n",
    "- All words get projected onto the same dimension\n",
    "- Best performance when given 4 past and 4 future words, and classifier is asked to guess the present word\n",
    "\n",
    "Skip gram\n",
    "\n",
    "- tries to maximize classification of words nearby\n",
    "\n",
    "# Results\n",
    "\n",
    "Task:\n",
    "\n",
    "- Generate word pairs manually with similarity\n",
    "- Create questions about the pairs\n",
    "- Question is answered correctly if the closest word to the vector is th exact word in the question\n",
    "\n",
    "By increasing dimension, can answer subtle semantic relationships\n",
    "\n",
    "2. Maximization of accuracy\n",
    "\n",
    "# Intuition\n",
    "\n",
    "These embedding models are all about training it on some task that will match most similar words to each other.\n",
    "For CBOW, we input 4 future and 4 past words and classifier is to guess the present word. This co-occureence of words allows for semantic similarities between words.\n",
    "\n",
    "# Phrase2Vec\n",
    "\n",
    "- indifference to word order\n",
    "\n",
    "> Distributed representations of words in a vector space help learning algorithms to achieve better performance in natural language processing tasks by grouping similar words.\n",
    "\n",
    "Its all about grouping words together in a compact space,\n",
    "\n",
    "> Word representations are limited by their inability to represent idiomatic phrases that are not com- positions of the individual words.\n",
    "\n",
    "Boston Globe should be one word, not two. Phrases like this are treated as separate words. Using vectors to represent the whole phases makes it more expressive\n",
    "\n",
    "# The Skip Gram Model\n",
    "\n",
    "The objective of the model is to maximize the log probability of the context words around window c given the target word.\n",
    "\n",
    "$\\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p\\left(w_{t+j} \\mid w_t\\right)$\n",
    "\n",
    "## 2.1 Hierarchical Softmax\n",
    "\n",
    "> The hierarchical softmax uses a binary tree representation of the output layer with the W words as its leaves and, for each node, explicitly represents the relative probabilities of its child node\n",
    "\n",
    "1. We start off with a 1-of-V one hot encoded vector with V inputs\n",
    "2. The projection layer is a V x D matrix, mapping each vector to a D-dimensional vector\n",
    "3. The output layer dot products target word with EACH embedding vector, and passes into softmax to calculate how likely the word is related to the target word (aka in the context window)\n",
    "\n",
    "The dot product indicates how similar the word is to target. angle between vectors should be near 0 if similar. So the model will push the context vectors being trained near target words to be closer and closer to each other.\n",
    "\n",
    "Heirarchical softmax is a binary tree of output layer with W words as leaves. Higher nodes indicate joint probabilities of many words. So instead of computing probabilities or softmaxes for each word, we only have to compute it for log(W) words.\n",
    "\n",
    "## 2.2 Negative Sampling\n",
    "\n",
    "$\\log \\sigma\\left(v_{w_0}^{\\prime}{ }^T v_{w_I}\\right)+\\sum_{i=1}^k \\mathbb{E}_{w_i \\sim P_n(w)}\\left[\\log \\sigma\\left(-v_{w_i}^{\\prime}{ }^T v_{w_I}\\right)\\right]$\n",
    "\n",
    "Essentially we want to maximize the log probability of the dot products of the target and context word embeddings.\n",
    "\n",
    "NCE: For k randomly sampled negative words, we want to maximize the opposite of the target and context word embeddings (negative). Similar to contrastive learning\n",
    "\n",
    "- needs numerical probabilities of the noise distribution\n",
    "\n",
    "## 2.3 Subsampling of Frequent Words\n",
    "\n",
    "Frequent words provide less information. To counter the balance between rare and frequent words, we simply discard words that are too frequent.\n",
    "\n",
    "$P\\left(w_i\\right)=1-\\sqrt{\\frac{t}{f\\left(w_i\\right)}}$\n",
    "\n",
    "So more frequent, the lower the probability of being in the training set.\n",
    "\n",
    "Negative Sampling is better than NCE and Heirarchical Softmax. It is faster and more accurate.\n",
    "Subsampling of frequent words makes training faster\n",
    "\n",
    "# Learning Phrases\n",
    "\n",
    "Important to note effective phrases are unigram and bigram. If a word is too rare, then we shouldnt include.\n",
    "\n",
    "$\\operatorname{score}\\left(w_i, w_j\\right)=\\frac{\\operatorname{count}\\left(w_i w_j\\right)-\\delta}{\\operatorname{count}\\left(w_i\\right) \\times \\operatorname{count}\\left(w_j\\right)}$\n",
    "\n",
    "This score is used to quantitatively measure how frequent the words in a phrase are\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
