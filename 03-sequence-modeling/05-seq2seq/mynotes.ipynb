{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intution\n",
    "\n",
    "Encoder-Decoder model presented a two part model for machine translation. The Seq2seq model was subsequently developed, using LSTM version architecture.\n",
    "\n",
    "# Encoder-Decoder\n",
    "\n",
    "> RNN Encoder–Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase\n",
    "\n",
    "builds upon word2vec, learning a distributed representation space. Except this is creating good representation for a sentence. And then the embeddings are rexpressed in a target language.\n",
    "\n",
    "# 2 RNN Encoder Decoder\n",
    "\n",
    "In RNNs, we can compute the probability of the next word as shown:\n",
    "\n",
    "$p\\left(x_{t, j}=1 \\mid x_{t-1}, \\ldots, x_1\\right)=\\frac{\\exp \\left(\\mathbf{w}_j \\mathbf{h}_{\\langle t\\rangle}\\right)}{\\sum_{j^{\\prime}=1}^K \\exp \\left(\\mathbf{w}_{j^{\\prime}} \\mathbf{h}_{\\langle t\\rangle}\\right)}$\n",
    "\n",
    "where $\\mathbf{h}_{\\langle t\\rangle}$ is the hidden state of the RNN at time step $t$.\n",
    "\n",
    "Notice how we take the dot product of the hidden state and the weight vector of the output layer. We multiply by the jth weight row because we want to output the jth word. x_t,j is the probability of the jth word at time t.\n",
    "\n",
    "$p(\\mathbf{x})=\\prod_{t=1}^T p\\left(x_t \\mid x_{t-1}, \\ldots, x_1\\right)$\n",
    "\n",
    "We multiply the probabilities of each word to get the probability of the sentence.\n",
    "\n",
    "## 2.2\n",
    "\n",
    "Propose to build a encoder and decoder network that can encode a variable length sequence into a fixed length vector, the decode back into variable length sequence. It's to learn the conditional distribution of a target sequence given a source sequence. (Like machine translation)\n",
    "\n",
    "> The encoder is an RNN that reads each symbol of an input sequence x sequentially\n",
    "\n",
    "> The decoder of the proposed model is another RNN which is trained to generate the output sequence by predicting the next symbol yt given the hidden state h. owever, unlike the RNN described in Sec. 2.1, both yt and h〈t〉 are also conditioned on yt−1 and on the summary c of the input sequence.\n",
    "\n",
    "We want to maximize the log likelihood, GIVEN the source input, what is the probability of the target output.\n",
    "\n",
    "$\\max _{\\boldsymbol{\\theta}} \\frac{1}{N} \\sum_{n=1}^N \\log p_{\\boldsymbol{\\theta}}\\left(\\mathbf{y}_n \\mid \\mathbf{x}_n\\right)$\n",
    "\n",
    "This idea of formulating the probability of some target appearing given source is exactly like text translation, where we are generating text\n",
    "\n",
    "# 2.3 Hidden Unit that Adaptively Remembers and Forget\n",
    "\n",
    "Adds a reset and update gate to unit in RNN\n",
    "\n",
    "# Statistical Machine Translation\n",
    "\n",
    "## Scoring Phrase Pairs with RNN Encoder–Decoder\n",
    "\n",
    "> Here we propose to train the RNN Encoder–Decoder (see Sec. 2.2) on a table of phrase pairs and use its scores as additional features in the log-linear model in Eq. (9)\n",
    "\n",
    "Let encoder-decoder produce scores (log likelihood) for phrase pairs. What does it mean to use the score as features?\n",
    "\n",
    "The score of a word being the translation given the source is a feature that contributes to how good a translation is. This scoring will be used for training data in SMT model and will be considered as a feature in the SMT deciison\n",
    "\n",
    "# Experiments\n",
    "\n",
    "Build English to French SMT model\n",
    "\n",
    "# Seq2Seq\n",
    "\n",
    "A key is reversing the source sequence of words, leading to the LSTM tracking short term dependencies, and being able to handle longer sentences.\n",
    "\n",
    "LSTMs have the property of mapping a variable sentence to a fixed vector representation\n",
    "\n",
    "# 2 Model\n",
    "\n",
    "> it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships\n",
    "\n",
    "RNNs can map sequences ahead of time if lengths are equal. however for uneven lengths this is difficult.\n",
    "\n",
    "The way it works:\n",
    "(x1,....xn) -> LSTM -> (h1,...hn) the hidden state -> LSTM -> (y1,...ym)\n",
    "Can be modeled as a conditional probability, each token we process will become another condition, and compute the next one\n",
    "\n",
    "$p\\left(y_1, \\ldots, y_{T^{\\prime}} \\mid x_1, \\ldots, x_T\\right)=\\prod_{t=1}^{T^{\\prime}} p\\left(y_t \\mid v, y_1, \\ldots, y_{t-1}\\right)$\n",
    "\n",
    "We are formulating a distirbution, given a source tokens, and the eventual output of a hidden vector v, compute how likely a yt token will be. Notice how its conditioned only on the previous tokens from the source and the hidden vector\n",
    "\n",
    "The reasons they used 2 LSTMs\n",
    "\n",
    "1. Using an LSTM that processed the input and one that processed the translation increases the parameters with negligible computational cost\n",
    "2. Deeper LSTMs better at learning than shallow ones.\n",
    "3. Reversing source sequence enhanced the translation because the first source word is closest to the first translation word and thus can learn shorter term dependencies\n",
    "\n",
    "# 3 Training\n",
    "\n",
    "## 3.2 Decoding and Rescoring\n",
    "\n",
    "Goal is maximizing log likelihood: $1 /|\\mathcal{S}| \\sum_{(T, S) \\in \\mathcal{S}} \\log p(T \\mid S)$\n",
    "\n",
    "Beam search is used to find the most likely outputs given the current most likely hypotheses (previous outputs in sequence)\n",
    "\n",
    "## Reversing the Source\n",
    "\n",
    "> Thus, backpropagation has an easier time “establishing communication” between the source sentence and the target sentence, which in turn results in substantially improved overall performance\n",
    "\n",
    "# Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
