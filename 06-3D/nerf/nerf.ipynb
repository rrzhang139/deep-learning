{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jaidevshriram/NeRF\n",
    "\n",
    "Neural Radiance Field (NeRF)\n",
    "\n",
    "# Introduction\n",
    "Develops an algorithm that takes 3D coordinates (x,y,z) and viewing direction (angle, phi), output is volume density and emitted radiance (color)\n",
    "\n",
    "The network is trained on a set of input images with known camera angles. It tries to minimize the difference between rendered view and the input images\n",
    "\n",
    "Can query a location and angle, and outputs a novel view. \n",
    "\n",
    "Represents the scene implicitly. Allows continuous high resolution\n",
    "\n",
    "# Intuition\n",
    "From what I understand, a camera ray samples a set of points, each with a location, and given the angle and pose, we output the color and density AT EACH point.\n",
    "We sample sparsely at the same angle because it generates consistency for the network to learn representations of hte same angle.\n",
    "\n",
    "While its training, the network learns the information for what should be displayed at a given point. The input image gives the spatial coordinates, and the network can layer its details on that location, and store it in weights. \n",
    "\n",
    "When inference comes, we query with coordinates and angle, and it learns a specific pathway that neurons take for a particular image that has similar inputs. \n",
    "\n",
    "Because NeRFs are differentiable neural networks, they backpropogate against a loss function that is continually checking its color and densities match the ones of input images.\n",
    "\n",
    "![Screenshot 2024-06-10 at 2.10.43â€¯PM.png](../../images/Screenshot_2024-06-10_at_2.10.43_PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
