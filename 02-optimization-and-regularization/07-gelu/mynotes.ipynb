{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Gelu Formation\n",
    "\n",
    "> We motivate our activation function by combining properties from dropout, zoneout, and ReLUs. First note that a ReLU and dropout both yield a neuron's output with the ReLU deterministically multiplying the input by zero or one and dropout stochastically multiplying by zero.\n",
    "\n",
    "> 2016). We merge this functionality by multiplying the input by zero or one, but the values of this zero-one mask are stochastically determined while also dependent upon the input. Specifically, we can multiply the neuron input \\(x\\) by \\(m \\sim \\operatorname{Bernoulli(}(\\Phi(x))\\), where \\(\\Phi(x)=P(X \\leq\\) \\(x), X \\sim \\mathcal{N}(0,1)\\) is the cumulative distribution function of the standard normal distribution. We choose this distribution since neuron inputs tend to follow a normal distribution, especially with Batch Normalization.\n",
    "\n",
    "So if the value x is high, the probability of not dropping it is high.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training activation: gelu, Data Type: normal\n",
      "Epoch: 999, Loss: 1.8292492853788644e-08\n",
      "Training activation: relu, Data Type: normal\n",
      "Epoch: 999, Loss: 2.05539754595703e-13\n",
      "Training activation: gelu, Data Type: uniform\n",
      "Epoch: 999, Loss: 6.628273752085079e-08\n",
      "Training activation: relu, Data Type: uniform\n",
      "Epoch: 999, Loss: 4.75004313660321e-10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, activation_type=\"gelu\"):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(input_size, hidden_size)\n",
    "        if activation_type == \"gelu\":\n",
    "            self.act = nn.GELU()\n",
    "        else:\n",
    "            self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, training_set):\n",
    "    model.train()\n",
    "    for epoch in range(1000):\n",
    "        loss = 0\n",
    "        for x, target in training_set:\n",
    "            optimizer.zero_grad()\n",
    "            x = model(x)\n",
    "            loss += criterion(x, target)\n",
    "            loss.backward()  # retain_graph=True\n",
    "            optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss / len(training_set)}\")\n",
    "\n",
    "    # return loss / target_length\n",
    "\n",
    "\n",
    "def generate_data(batch_size, input_size):\n",
    "    # 1. Normally distributed data\n",
    "    normal_input = torch.randn(batch_size, input_size)\n",
    "    normal_target = torch.randn(batch_size, 1)\n",
    "    # 2. Uniform distributed data [-1, 1]\n",
    "    uniform_input = torch.rand(batch_size, input_size) * 2 - 1\n",
    "    uniform_target = torch.rand(batch_size, 1) * 2 - 1\n",
    "\n",
    "    return normal_input, normal_target, uniform_input, uniform_target\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "input_size = 64\n",
    "lr = 0.001\n",
    "\n",
    "# input: (batch_size, input_size)\n",
    "# output: (batch_size, 1)\n",
    "\n",
    "normal_input, normal_target, uniform_input, uniform_target = generate_data(\n",
    "    batch_size, input_size)\n",
    "\n",
    "for input, target, data_type in [(normal_input, normal_target, \"normal\"), (uniform_input, uniform_target, \"uniform\")]:\n",
    "    for activation_type in [\"gelu\", \"relu\"]:\n",
    "        print(f\"Training activation: {\n",
    "              activation_type}, Data Type: {data_type}\")\n",
    "        # Instantiate the model\n",
    "        model = NeuralNetwork(input_size=input_size,\n",
    "                              hidden_size=64, activation_type=activation_type)\n",
    "\n",
    "        # Define the loss function\n",
    "        # Why not Cross entropy? because output is not a d-dim vector we map to a label. not trying to find\n",
    "        # highest likelihood of a class, just trying to minimize loss\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # Define the training set\n",
    "        training_set = DataLoader(\n",
    "            TensorDataset(input, target), batch_size=batch_size)\n",
    "\n",
    "        # Call the train function\n",
    "        train(model, criterion, optimizer, training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for 10 epochs, we can see from the results that the Gelu activation on normal data performs almost 2x better with half of its loss. (0.1527 < 0.293)\n",
    "Training for 100 epochs,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
