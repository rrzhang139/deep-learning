{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Adding inductive biases into the model is a good way to generalize well for a specific task.\n",
    "\n",
    "A feature map is the output representation. E.g 64 units in a feature map is an output representation containing 64 dimensions. A unique kernel generates this feature map.\n",
    "So if we say there are 12 feature maps, there are {{c1::12}} kernels.\n",
    "\n",
    "A CNN has a few properties and attributes:\n",
    "\n",
    "1. Local connectivity: Each neuron is connected to a small region of the input volume.\n",
    "2. Shared weights: Each neuron in a feature map shares the same weights, reducing the number of parameters.\n",
    "\n",
    "# The Computation of a Convolutional Layer\n",
    "\n",
    "The computation of a convolutional layer is as follows:\n",
    "\n",
    "1. Start at H1 The input volume is convoled with a set of kernels. Each kernel creates a feature map. There are 12 feature maps. Each feature map contains N=64 units, or output activations.\n",
    "2. The feature maps are stacked to form the output volume. (WxHxD), where D is the number of feature maps.\n",
    "3. From H1, we can calculate the how many units we used (12 feature maps x 64 units each = 768 units). Calculating how many connections (remember each output unit is the result of dot product of input/pixel _ weight, and ther eare 25 weights in a kernel, 25 _ 768 = 19200 connections). How many total free parameters are there? (768 biases (each output gets a bias) + 25 weights in a kernel x 12 kernels)(remember each weight is a param, bias is param given for every output).\n",
    "   -We have a bias for every output unit for activation threshold adjustment independent of the input.\n",
    "4. Layer 2 called H2, has 12 feature maps. Each featuer map has 16 units.\n",
    "   -So Each unit in H2 combiens 8 of 12 feature maps in H1.\n",
    "   -The kernel dimensions are 5x5\n",
    "5. From H2, how many units we used\n",
    "\n",
    "## Experimental Setup\n",
    "\n",
    "- Used MSE as the loss function\n",
    "- Weights randomly initialized using a\n",
    "- Used stochastic gradient descent as the optimizer. This means that the weights are updated after each batch of data is passed through the network. Redundancy in data helps SGD converge faster because it sees the same examples.\n",
    "- More efficient on larger datasets, as it can update the weights more frequently and generalize better (due to randomness in selecting the sample).\n",
    "\n",
    "## Results\n",
    "\n",
    "Backprop is effective due to redundancy in data\n",
    "\n",
    "- Presented many missclassifications due to poor writing styles\n",
    "-\n",
    "\n",
    "# LeNet\n",
    "\n",
    "Pattern recignition can be learned by automated learning through backpropogation\n",
    "\n",
    "##Introduction\n",
    "\n",
    "- Originally, we solve pattern recognition using automated tasks and manual algorithms. The basic method is to feature extract\n",
    "  3 Reasons we have improved since then:\n",
    "\n",
    "1. improved machine learning techniques\n",
    "2. larger databases containing real data used for necessary downstream tasks\n",
    "3. Powerful computers, low cost, fast arithmetic for numerical methods rather than algorithmic methods\n",
    "\n",
    "## B. Gradient-Based Learning\n",
    "\n",
    "- Easier to minimize a smooth continuous function\n",
    "- Specifically, the Stochastic Gradient Descent\n",
    "- W is updated based on a single sample\n",
    "\n",
    "## C. Gradident Backpropogation\n",
    "\n",
    "3 events occured that popularized gradient based learning:\n",
    "\n",
    "1. Local minima are not a problem in practice, after Botlzmann machines were introduced\n",
    "2. Backpropogation: copmute the gradient in a non-linear system with several layers\n",
    "3. Multi-layer neural networks and sigmoid activation functions solved complicated learning tasks\n",
    "\n",
    "## D. Learning Real Handwritten Recognition Systems\n",
    "\n",
    "- There comes a challenge when having to recognize digits. The difficulty is often segmenting characters from the words,\n",
    "\n",
    "## E. Globally Trainable Systems\n",
    "\n",
    "- Usually to identify handwritten digits requires multiple modules: field locator, segmenter, recognizer (classifies and scores each candidate character). But each module has to be separately trained\n",
    "- Alternative is to assign a global error function to the entire system\n",
    "\n",
    "# CNNs\n",
    "\n",
    "Problems with feeding entire raw images into a neural network:\n",
    "\n",
    "- The number of parameters in the network can be very large\n",
    "- no built-in invariance with resp ect to translations or local distortions of the inputs\n",
    "\n",
    "## A. Convolutional Neural Networks\n",
    "\n",
    "> Convolutional Networks combine three architectural ideas to ensure some degree of shift, scale, and distortion invariance: local receptive fields, shared weights (or weight replication), and spatial or temporal sub-sampling.\n",
    "> Feature Maps:\n",
    "\n",
    "- Each unit in a feature map is connected to a local region of the input, and all units in a feature map share the same set of weights (i.e., the filter).\n",
    "- However, a complete convolutional layer consists of multiple feature maps, each with its own set of weights (filters).\n",
    "\n",
    "Local Receptive fields\n",
    "\n",
    "- Each unit in a feature map is connected to a local region of the input, and all units in a feature map share the same set of weights (i.e., the filter).\n",
    "  > For example, in the first hidden layer of LeNet-5, the receptive fields of horizontally contiguous units overlap by 4 columns and 5 rows. As stated earlier, all the units in a feature map share the same set of 25 weights and the same bias so they detect the same feature at all possible locations on the input. The other feature maps in the layer use different sets of weights and biases, thereby extracting different types of local features.\n",
    "- Sub-sampling (Pooling) layers are used to reduce the spatial resolution of the feature maps to reduce senstivity of output to shifts and invariances\n",
    "\n",
    "As we move deeper in LeNet layers, the number of feature maps increases and the spatial resolution is decreased so we can achieve invariance to geometric distortions, while increasing richness of the representation\n",
    "\n",
    "## B. LeNet-5\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
