{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition\n",
    "\n",
    "# BERT\n",
    "\n",
    "BERT is built for finetuning tasks\n",
    "\n",
    "## Input/Output Representations\n",
    "\n",
    "> Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., \\(\\langle\\) Question, Answer \\(\\rangle\\) ) in one token sequence.\n",
    "\n",
    "## 3.1 Pre-training BERT\n",
    "\n",
    "> Task \\#1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model.\n",
    "\n",
    "But the problem was the model bidirectionally conditioned will see tokens ahead of time and thus just predict that token with high probability.\n",
    "\n",
    "Instead, we mask a percentage of the tokens randomly, and then predict those tokens. This is now unsupervised, since we don't have the labels for the masked tokens. Rather, we are using the natural structure of the input data to predict the next token.\n",
    "\n",
    "> Task \\#2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling.\n",
    "\n",
    "This is a classification problem, labels InNext or NotNext. A [CLS] token is prepended before every sequence, and since a sequence can define a pair of sentences or sentence, the [CLS] token can be used to predict the next sentence.\n",
    "\n",
    "## 3.2 Fine-tuning BERT\n",
    "\n",
    "Naturally, the way to finetune a transformer is providing task-specific inputs (like special tokens) and outputs (e.g classification maybe require adding softmax layer on top)\n",
    "Transfoemers are nice since they provide cross-attention weights\n",
    "\n",
    "> At the output, the token representations are fed into an output layer for token level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis\n",
    "\n",
    "The BERT model is so versatile in finetuning where the output can represent token-level outputs, where decisions are made on individual tokens, or classification results where decicisions are made on the entire sequence. (CLS is aggregation or summarization of the entire sequence)\n",
    "\n",
    "# 4 Experiments\n",
    "\n",
    "## Glue\n",
    "\n",
    "# RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
    "\n",
    "We find that BERT was significantly undertrained,\n",
    "present a set of important BERT design choices and training strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
