{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition\n",
    "\n",
    "I wonder how the innovations from word vectors to sequence modeling to BERT becoming so popular in the pretraining paradigm of generalpurpose knowledge is attributable to the fact that language and words are naturally structured and have properties that make it scale quick (large data -> generalized architectures (like Transformers) that soak up the data and learn a lot of information about the world).\n",
    "Relationships between data seem extremely important, but it must be soaked in a medium with high proximity to usage and context (transformers utilize this well)\n",
    "Why do Knowledge Graphs not work? unscalable?\n",
    "\n",
    "Flexibility of tasks and the pre-training/fine-tuning paradigm is what scales NLP. Tasks are standardized and models are trained on a lot of data. This is a good way to learn a lot of information about the world.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Interesting how pretrained models are trained on DATARICH tasks (predict next word in sequence given previous words). This is an unsupervised task, but it is trained on a lot of data. This is a good way to learn a lot of information about the world.\n",
    "Finetuning is for downstream tasks\n",
    "\n",
    "> instead, it is often learned as part of an auxiliary task. For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors. These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).\n",
    "\n",
    "Originally, NLP started with traning word vectors as a way to represent words such that it gains nice properties like co-occurences.\n",
    "\n",
    "> Ideally, this pre-training causes the model to develop general-purpose abilities\n",
    "\n",
    "Interesting how in CV they still use supervised tasks, and today with ViTs it draws so much influence and performance gains to just be unsupervised. CV has unsupervised tasks too like masking.\n",
    "\n",
    "> The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem\n",
    "\n",
    "This simplification has probably given huge efficiency gains in the field. Standardization means benchmarks are consistent across projects, data is easier to manage, and models are easier to compare. It also doesnt require training whole new architectures for every new task.\n",
    "\n",
    "# Setup\n",
    "\n",
    "> For our model, we use a standard encoder-decoder Transformer\n",
    "> In order to compare a variety of tasks, a baseline model is used, and altering one aspect at a time\n",
    "> Our baseline model is designed so that the encoder and decoder are each similar in size and configuration to a “BERTBASE”\n",
    "\n",
    "# Takeaways\n",
    "\n",
    "> Text-to-text Our text-to-text framework provides a simple way to train a single model on a wide variety of text tasks using the same loss function and decoding procedure. We showed how this approach can be successfully applied to generative tasks like abstractive summarization, classification tasks like natural language inference, and even regression tasks like STS-B. In spite of its simplicity, we found the text-totext framework obtained comparable performance to task-specific architectures and ultimately produced state-of-the-art results when combined with scale.\n",
    "\n",
    "Text to text is generalized, but matches performance for task specific architectures.\n",
    "\n",
    "> Architectures: While some work on transfer learning for NLP has considered architectural variants of the Transformer, we found the original encoderdecoder form worked best in our text-to-text framework.\n",
    "\n",
    "Classic transformer encoder-decoder is best for transer learning\n",
    "\n",
    "> Unsupervised Objectives: We suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient.\n",
    "\n",
    "Seems like we prioritize practical efficiency (shorter training objectives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
